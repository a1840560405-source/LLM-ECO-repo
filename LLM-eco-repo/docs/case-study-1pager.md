# 1 页案例（Case Study, 1‑pager）

**项目**：文本信号 → 可量化指标 → 简单因果评估（合成数据演示）  
**作者**：钟廷杰  
**日期**：2026-01-16  

> 说明：本页用仓库内的合成数据做演示，重点是方法与工程组织方式；不把结论当作真实业务结论。

---

## 背景问题

政策/公告/研报/财报里有大量“定性信息”，但要把它变成可用的研究变量，通常会遇到三个麻烦：

1) 文本太碎：同一家公司、同一事件往往散落在不同文本里；
2) 指标不好解释：做出来的 embedding 或主题模型，讲不清到底在测什么；
3) 评估容易踩坑：例如时间泄漏（用到未来信息）、代理变量偏误（measurement error）导致回归系数偏小。

---

## 数据与最小闭环

- **文本侧**：政策/公告片段（`data/sample/policy_texts.csv`）与财报摘要（`data/sample/financial_reports.csv`）
- **结构化侧**：公司年度财务指标（收入、利润等）

闭环流程：

1) 文本对齐（检索）：用 TF‑IDF + cosine 做“财报摘要 → 政策文本”的最相近匹配（类 RAG 的最小版本）；
2) 指标构建：关键词情绪计数 + 低维语义特征（SVD on TF‑IDF）；
3) 代理变量：把“真实但不可见的潜变量”加噪得到 proxy；
4) EV 校正：用一小部分“人工标注”（模拟）估计衰减系数并做校正；
5) DID：用 pre‑2020 收入定义 treated，避免用到 post 信息，再做固定效应回归。

---

## 结果长什么样（示例）

- 能拿到一张由 pipeline 产出的、可直接入回归的数据表（带文本特征、匹配分数、校正后的 proxy）。
- DID 的 `treated_post` 系数可跑通（注意：合成数据只验证流程，不证明现实关系）。

---

## 我刻意做的“更专业”的处理

- **中文 tokenization**：不依赖外部分词库，用“字符 token + 空白 token”的混合 tokenizer，让 TF‑IDF 不至于在中文上失效。
- **小样本稳健性**：SVD 维度自适应，避免词表太小导致报错。
- **EV 校正安全阈值**：衰减系数过小会放大噪声，直接回退到未校正的 proxy 并记录 warning，保证可审计。
- **防泄漏**：treated 的定义只用 pre‑period 信息。

---

## 局限与下一步

- 合成数据无法代表真实世界；真实项目必须做口径对齐、缺失/异常处理与合规审查。
- TF‑IDF 只是 baseline；如果要上强度，可替换为更稳的中文分词/向量化与更严格的对齐抽检。
- 评估方法可以扩展：事件研究、安慰剂检验、稳健标准误、异质性分析等。
