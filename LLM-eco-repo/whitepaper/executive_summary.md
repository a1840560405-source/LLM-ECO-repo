# 执行摘要

这份白皮书关注一个很现实的问题：文本类数据越来越多，但研究里最敏感的两件事——**识别**和**复现**——不能因为“上了模型”就放松。

我在文中想讲清楚三点：

1) **LLM 更像特征工具，不是替代经济学设计**。它可以帮你更快生成文本指标，但识别策略（IV、DID、RDD 等）和稳健性检查仍然要回到计量框架里。
2) **工程链路要可追溯**。无论你用 TF‑IDF 还是 embedding/RAG，最终都要回答：文本怎么对齐到样本？指标怎么定义？版本/参数怎么记录？
3) **需要承认并处理“代理变量”问题**。文本指标通常是 proxy，测量误差会带来系数衰减；抽样标注 + EV 校正是一个成本可控、但很有效的补丁。

行动建议（更偏实操）：先用小规模样本把口径/时间窗定好，跑通一条可复现的 pipeline；再逐步替换模块（tokenizer/embedding/检索）并做敏感性分析。本文附带的仓库 demo 主要用于展示流程，不把合成数据上的回归结果当成现实结论。